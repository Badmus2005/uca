# ü§ñ Guide d'export et d√©ploiement du mod√®le

## üì¶ Apr√®s l'entra√Ænement

Une fois votre mod√®le entra√Æn√© (`best.pt`), vous devez l'**exporter** au bon format selon votre plateforme cible.

## üîÑ Formats disponibles

| Format | Extension | Plateforme | Utilisation |
|--------|-----------|------------|-------------|
| **PyTorch** | `.pt` | Python + PyTorch | ‚úÖ Format natif (d√©j√† disponible) |
| **ONNX** | `.onnx` | Multi-plateforme | ‚úÖ C++, C#, Java, JavaScript, Python |
| **TFLite** | `.tflite` | Mobile/Embarqu√© | ‚úÖ Raspberry Pi, Android, iOS, Microcontr√¥leurs |
| **TorchScript** | `.torchscript` | C++ LibTorch | ‚úÖ D√©ploiement C++ |
| **CoreML** | `.mlmodel` | iOS/macOS | ‚úÖ Apple optimis√© |
| **TensorFlow** | `.pb` | TensorFlow | ‚úÖ Serveurs, Cloud |

## üöÄ Export rapide

### M√©thode 1 : Script interactif (RECOMMAND√â)
```powershell
python scripts/export_model.py
```
Suivez les instructions √† l'√©cran.

### M√©thode 2 : Export manuel
```powershell
cd models/yolov5
python export.py --weights ../trained_models/garbage_classifier_v1/weights/best.pt --include onnx tflite --img 640
```

## ü§ñ Pour votre DOFBot

### Option A : Raspberry Pi / Linux embarqu√©
**Format recommand√© : TFLite**

```python
# Installation
pip install tensorflow-lite

# Utilisation
import numpy as np
import tflite_runtime.interpreter as tflite

# Charger le mod√®le
interpreter = tflite.Interpreter(model_path="best.tflite")
interpreter.allocate_tensors()

# Inf√©rence
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Pr√©parer image (640x640)
input_data = np.array(image, dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)

# Ex√©cuter
interpreter.invoke()

# R√©cup√©rer r√©sultats
output_data = interpreter.get_tensor(output_details[0]['index'])
```

### Option B : NVIDIA Jetson
**Format recommand√© : ONNX + TensorRT**

```python
# Installation
pip install onnxruntime-gpu

# Utilisation
import onnxruntime as ort

session = ort.InferenceSession("best.onnx", providers=['CUDAExecutionProvider'])
results = session.run(None, {'images': input_data})
```

### Option C : PC Windows/Linux avec Python
**Format : PyTorch (.pt) - Aucun export n√©cessaire**

```python
import torch

model = torch.hub.load('ultralytics/yolov5', 'custom', path='best.pt')
results = model('image.jpg')
results.show()
```

## üìä Tailles des mod√®les

| Mod√®le | Taille .pt | Taille ONNX | Taille TFLite |
|--------|-----------|-------------|---------------|
| YOLOv5s | ~14 MB | ~28 MB | ~15 MB |
| YOLOv5m | ~40 MB | ~80 MB | ~42 MB |
| YOLOv5l | ~90 MB | ~180 MB | ~95 MB |

## üîß D√©pendances pour l'export

```powershell
# Pour ONNX
pip install onnx onnx-simplifier

# Pour TFLite
pip install tensorflow

# Pour CoreML (macOS seulement)
pip install coremltools

# Pour TensorRT (Jetson)
# Pr√©-install√© sur Jetson avec JetPack
```

## üí° Conseils de d√©ploiement

### Performance
- **TFLite** est le plus rapide sur CPU/ARM
- **TensorRT** est le plus rapide sur GPU NVIDIA
- **ONNX** offre le meilleur compromis multi-plateforme

### Taille m√©moire
- **TFLite** : le plus compact
- **ONNX** : taille moyenne
- **PyTorch** : le plus volumineux

### Facilit√© d'int√©gration
- **PyTorch** : tr√®s facile (Python uniquement)
- **ONNX** : facile (multi-langages)
- **TFLite** : moyen (n√©cessite preprocessing)

## üéØ Recommandation pour DOFBot

**Utilisez TFLite** pour :
- ‚úÖ Meilleure performance sur ARM (Raspberry Pi)
- ‚úÖ Taille m√©moire r√©duite
- ‚úÖ Latence faible

**Commande rapide :**
```powershell
python scripts/export_model.py
# Choisir option 2 (TFLite)
```

## üìö Ressources

- [YOLOv5 Export Documentation](https://github.com/ultralytics/yolov5/tree/master#export)
- [ONNX Runtime](https://onnxruntime.ai/)
- [TensorFlow Lite Guide](https://www.tensorflow.org/lite/guide)
- [TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/)
